[
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Introducing Strands Agents, an Open Source AI Agents SDK by Clare Liguori | on 16 MAY 2025 | in Announcements, Artificial Intelligence, Geneative AI, Open Source | Permalink | Comments | Shared\nToday I am happy to announce we are releasing Strands Agents. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production. Multiple teams at AWS already use Strands for their AI agents in production, including Amazon Q Developer, AWS Glue, and VPC Reachability Analyzer. Now, I’m thrilled to share Strands with you for building your own AI agents.\nCompared with frameworks that require developers to define complex workflows for their agents, Strands simplifies agent development by embracing the capabilities of state-of-the-art models to plan, chain thoughts, call tools, and reflect. With Strands, developers can simply define a prompt and a list of tools in code to build an agent, then test it locally and deploy it to the cloud. Like the two strands of DNA, Strands connects two core pieces of the agent together: the model and the tools. Strands plans the agent’s next steps and executes tools using the advanced reasoning capabilities of models. For more complex agent use cases, developers can customize their agent’s behavior in Strands. For example, you can specify how tools are selected, customize how context is managed, choose where session state and memory are stored, and build multi-agent applications. Strands can run anywhere and can support any model with reasoning and tool use capabilities, including models in Amazon Bedrock, Anthropic, Ollama, Meta, and other providers through LiteLLM.\nStrands Agents is an open community, and we’re excited that several companies are joining us with support and contributions including Accenture, Anthropic, Langfuse, mem0.ai, Meta, PwC, Ragas.io, and Tavily. For instance, Anthropic has already contributed support in Strands for using models through the Anthropic API, and Meta contributed support for Llama models through Llama API. Join us GitHub to get started with Strands Agents!\nOur journey building agents I primarily work on Amazon Q Developer, a generative AI-powered assistant for software development. My team and I started building AI agents in early 2023, around when the original ReAct (Reasoning and Acting) was published. This paper showed that large language models could reason, plan, and take actions in their environment. For example, LLMs could reason that they needed to make an API call to complete a task and then generate the inputs needed for that API call. We then realized that large language models could be used as agents to complete many types of tasks, including complex software development and operational troubleshooting.\nAt that time, LLMs weren’t typically trained to act like agents. They were often trained primarily for natural language conversation. Successfully using an LLM to reason and act required complex prompt instructions on how to use tools, parsers for the model’s responses, and orchestration logic. Simply getting LLMs to reliably produce syntactically correct JSON was a challenge at the time! To prototype and deploy agents, my team and I relied on a variety of complex agent framework libraries that handled the scaffolding and orchestration needed for the agents to reliably succeed at their tasks with these earlier models. Even with these frameworks, it would take us months of tuning and tweaking to get an agent ready for production.\nSince then, we’ve seen a dramatic improvement in large language models’ abilities to reason and use tools to complete tasks. We realized that we no longer needed such complex orchestration to build agents, because models now have native tool-use and reasoning capabilities. In fact, some of the agent framework libraries we had been using to build our agents started to get in our way of fully leveraging the capabilities of newer LLMs. Even though LLMs were getting dramatically better, those improvements didn’t mean we could build and iterate on agents any faster with the frameworks we were using. It still took us months to make an agent production-ready.\nWe started building Strands Agents to remove this complexity for our teams in Q Developer. We found that relying on the latest models’ capabilities to drive agents significantly reduced our time to market and improved the end user experience, compared to building agents with complex orchestration logic. Where it used to take months for Q Developer teams to go from prototype to production with a new agent, we’re now able to ship new agents in days and weeks with Strands.\nCore concepts of Strands Agents The simplest definition of an agent is a combination of three things: 1) a model, 2) tools, and 3) a prompt. The agent uses these three components to complete a task, often autonomously. The agent\u0026rsquo;s task could be to answer a question, generate code, plan a vacation, or optimize your financial portfolio. In a model-driven approach, the agent uses the model to dynamically direct its own steps and to use tools in order to accomplish the specified task.\nTo define an agent with the Strands Agents SDK, you define these three components in code:\nModel: Strands offers flexible model support. You can use any model in Amazon Bedrock that supports tool use and streaming, a model from Anthropic’s Claude model family through the Anthropic API, a model from the Llama model family via Llama API, Ollama for local development, and many other model providers such as OpenAI through LiteLLM. You can additionally define your own custom model provider with Strands. Tools:You can choose from thousands of published Model Context Protocol (MCP) servers to use as tools for your agent. Strands also provides 20 công cụ ví dụ dựng sẵn, including tools for manipulating files, making API requests, and interacting with AWS APIs. You can easily use any Python function as a tool, by simply using the Strands @tool decorator. Prompt: You provide a natural language prompt that defines the task for your agent, such as answering a question from an end user. You can also provide a system prompt that provides general instructions and desired behavior for the agent. An agent interacts with its model and tools in a loop until it completes the task provided by the prompt. This agentic loop is at the core of Strands’ capabilities. The Strands agentic loop takes full advantage of how powerful LLMs have become and how well they can natively reason, plan, and select tools. In each loop, Strands invokes the LLM with the prompt and agent context, along with a description of your agent’s tools. The LLM can choose to respond in natural language for the agent’s end user, plan out a series of steps, reflect on the agent’s previous steps, and/or select one or more tools to use. When the LLM selects a tool, Strands takes care of executing the tool and providing the result back to the LLM. When the LLM completes its task, Strands returns the agent’s final result .\nIn Strands’ model-driven approach, tools are key to how you customize the behavior of your agents. For example, tools can retrieve relevant documents from a knowledge base, call APIs, run Python logic, or just simply return a static string that contains additional model instructions. Tools also help you achieve complex use cases in a model-driven approach, such as with these Strands Agents example pre-built tools:\nRetrieve tool: This tool implements semantic search using Amazon Bedrock Knowledge Bases. eyond retrieving documents, the retrieve tool can also help the model plan and reason by retrieving other tools using semantic search. For example, one internal agent at AWS has over 6,000 tools to select from! Models today aren’t capable of accurately selecting from quite that many tools. Instead of describing all 6,000 tools to the model, the agent uses semantic search to find the most relevant tools for the current task and describes only those tools to the model. You can implement this pattern by storing many tool descriptions in a knowledge base and letting the model use the retrieve tool to retrieve a subset of relevant tools for the current task. Thinking tool: This tool prompts the model to do deep analytical thinking through multiple cycles, enabling sophisticated thought processing and self-reflection as part of the agent. In the model-driven approach, modeling thinking as a tool enables the model to reason about if and when a task needs deep analysis. Multi-agent tools như like the workflow, graph, and swarm tools: For complex tasks, Strands can orchestrate across multiple agents in a variety of multi-agent collaboration patterns. By modeling sub-agents and multi-agent collaboration as tools, the model-driven approach enables the model to reason about if and when a task requires a defined workflow, graph, or swarm of sub-agents. Strands support for the Agent2Agent (A2A) protocol for multi-agent applications is coming soon. Get started with Strands Agents Let’s walk through an example of building an agent with the Strands Agents SDK. As has long been said, naming things is one of the hardest problems in computer science. Naming an open source project is no exception! To help us brainstorm potential names for the Strands Agents project, I built a naming AI assistant using Strands. In this example, you will use Strands to build a naming agent using a default model in Amazon Bedrock, an MCP server, and a pre-built Strands tool.\nCreate a file named agent.py with this code:\nfrom strands import Agent from strands.tools.mcp import MCPClient from strands_tools import http_request from mcp import stdio_client, StdioServerParameters # Define a naming-focused system prompt NAMING_SYSTEM_PROMPT = \u0026#34;\u0026#34;\u0026#34; You are an assistant that helps to name open source projects. When providing open source project name suggestions, always provide one or more available domain names and one or more available GitHub organization names that could be used for the project. Before providing your suggestions, use your tools to validate that the domain names are not already registered and that the GitHub organization names are not already used. \u0026#34;\u0026#34;\u0026#34; # Load an MCP server that can determine if a domain name is available domain_name_tools = MCPClient(lambda: stdio_client( StdioServerParameters(command=\u0026#34;uvx\u0026#34;, args=[\u0026#34;fastdomaincheck-mcp-server\u0026#34;]) )) # Use a pre-built Strands Agents tool that can make requests to GitHub # to determine if a GitHub organization name is available github_tools = [http_request] with domain_name_tools: # Define the naming agent with tools and a system prompt tools = domain_name_tools.list_tools_sync() + github_tools naming_agent = Agent( system_prompt=NAMING_SYSTEM_PROMPT, tools=tools ) # Run the naming agent with the end user\u0026#39;s prompt naming_agent(\u0026#34;I need to name an open source project for building AI agents.\u0026#34;) You will need a GitHub personal access token to run the agent. Set the environment variable GITHUB_TOKEN with the value of your GitHub token. You will also need Bedrock model access for Anthropic Claude 3.7 Sonnet in us-west-2, and AWS credentials configured locally.\nNow run your agent:\npip install strands-agents strands-agents-tools python -u agent.py You should see output from the agent similar to this snippet:\nBased on my checks, here are some name suggestions for your open source AI agent building project: ## Project Name Suggestions: 1. **Strands Agents** - Available domain: strandsagents.com - Available GitHub organization: strands-agents You can easily start building new agents today with the Strands Agents SDK in your favorite AI-assisted development tool. To help you quickly get started, we published a Strands MCP server to use with any MCP-enabled development tool, such as the Q Developer CLI or Cline. For the Q Developer CLI, use the following example to add the Strands MCP server to the CLI’s MCP Configuration. You can see more configuration examples on GitHub.\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;strands\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;strands-agents-mcp-server\u0026#34;] } } } Deploy Strands Agents in production Running agents in production is a key tenet for the design of Strands. The Strands Agents project includes a (deployment toolkit) with a set of reference implementations to help you take your agents to production. Strands is flexible enough to support a variety of architectures in production. You can use Strands to build conversational agents as well as agents that are triggered by events, run on a schedule, or run continuously. You can deploy an agent built with the Strands Agents SDK as a monolith, where both the agentic loop and the tool execution run in the same environment, or as a set of microservices. I will describe four agent architectures that we use internally at AWS with Strands Agents.\nThe following diagram shows an agent architecture with Strands running entirely locally in a user’s environment through a client application. The example command line tool GitHub follows this architecture for a CLI-based AI assistant for building agents.\nThe next diagram shows an architecture where the agent and its tools are deployed behind an API in production. We have provided reference implementations on GitHub for how to deploy agents built with the Strands Agents SDK behind an API on AWS, using AWS Lambda, AWS Fargate, or Amazon Elastic Compute Cloud (Amazon EC2).\nYou can separate concerns between the Strands agentic loop and tool execution by running them in separate environments. The following diagram shows an agent architecture with Strands where the agent invokes its tools via API, and the tools run in an isolated backend environment separate from the agent’s environment. For example, you could run your agent’s tools in Lambda functions, while running the agent itself in a Fargate container.\nYou can also implement a return-of-control pattern with Strands, where the client is responsible for running tools. This diagram shows an agent architecture where an agent built with the Strands Agents SDK can use a mix of tools that are hosted in a backend environment and tools that run locally through a client application that invokes the agent.\nRegardless of your exact architecture, observability of your agents is important for understanding how your agents are performing in production. Strands provides instrumentation for collecting agent trajectories and metrics from production agents. Strands uses OpenTelemetry (OTEL) to emit telemetry data to any OTEL-compatible backend for visualization, troubleshooting, and evaluation. Strands’ support for distributed tracing enables you to track requests through different components in your architecture, in order to paint a complete picture of agent sessions.\nJoin the Strands Agents community Strands Agents is an open source project licensed under the Apache License 2.0. We are excited to now build Strands in the open with you. We welcome contributions to the project, including adding support for additional providers’ models and tools, collaborating on new features, or expanding the documentation. If you find a bug, have a suggestion, or have something to contribute, join us on GitHub.\nTo learn more about Strands Agents and to start building your first AI agent with Strands, visit our documentation.\nClare Liguori Clare Liguori is a Senior Principal Software Engineer for AWS Agentic AI. She focuses on re-imagining how applications are built and how productive developers can be when their tools are powered by generative AI and AI agents, as part of Amazon Q Developer. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Introducing managed accounting for AWS Parallel Computing Service by Ramin Torabi, Nick Ihli, and Tarun Mathur | on 15 MAY 2025 | in AWS Parallel Computing Service , High Performance Computing, Technical How-to | Permalink | Share\nThis post was contributed by Ramin Torabi and Tarun Mathur from AWS, and Nick Ihli from SchedMD.\nAWS Parallel Computing Service (PCS) is a managed service that makes it easier for you to run and scale your high performance computing (HPC) workloads on AWS using Slurm. Organizations running HPC clusters want to monitor resource utilization, enforce resource limits, and manage access-control to specific capacity across users and projects. They want to understand “who did what” in their cluster for leadership reporting, capacity planning, and budgeting purposes. PCS now supports accounting, a Slurm feature that enables these activities in a cluster. PCS manages the accounting database for the cluster, so that you don’t have to setup and manage a separate accounting database.\nIn this post, we’ll show you how this works, and point you to some actual use cases you can try yourself.\nSetup Follow these steps to enable accounting in PCS:\nCreate a new PCS cluster with Slurm 24.11 or later, opt in to the accounting feature, and configure optional accounting parameters as shown in Figure-1. Once the cluster status is Active, verify accounting is enabled and review configured parameters in the cluster details console page. Configure and connect to a login node as described here, and perform accounting commands using the root account. Figure 1 – The “Create Cluster” console experience where you can enable and configure accounting. Use Case 1: Attribute Usage to Projects Organizations want visibility into resource usage for each project or department so they can internally chargeback relevant cost centers. To do so, they need to track and attribute usage at different levels. One scenario in Figure-2:\nCreate 3 users, create accounts proj\\_physics and proj\\_chemistry, and add users to accounts with the sacctmgr function. An ‘account’ is an organizational unit used to group and manage users. Each user submits a single job attributed to the proj\\_physics account with the –account= \u0026lt;account\\_name\u0026gt; flag. Validate that those jobs are attributed to the correct account proj_physics by looking up accounting data with the sacct function. Validate that user1 is a member of both proj\\_physics and proj\\_chemistry accounts. Figure 2 – Example workflow of project attribution. Use Case 2: Enforce Limits Organizations want to set constraints on particular users or projects so one party isn’t hoarding resources. One scenario:\nSet a limit of 6000 CPU minutes (100 CPU hours) for a particular user with the command sacctmgr modify user username set GrpTRESRunMins=cpu=6000 Run the validation in Figure-3 to check the limit was set correctly. Let’s assume the user has already used 95 CPU hours, and then tries to submit a job that would exceed their quota: sbatch --cpus-per-task=10 --time=1:00:00 myjob.sh. This job requests 10 CPUs for 1 hour, which would use 10 CPU hours, exceeding the remaining 5 CPU hours in the user’s quota. The job submission will fail and the user will see the error in Figure-4. The user can then submit a smaller job of say 4 CPU hours which would be accepted as it fits within the remaining quota: sbatch --cpus-per-task=2 --time=2:00:00 [smalljob.sh](http://smalljob.sh) Figure 3 – Example check to identify a limit was set correctly. Figure 4 – Example output of an error due to a job submission that exceeds user limit. Use Case 3: Generate Usage Reports Organizations want usage report summaries to assess their resource utilization and plan future capacity allocations. One scenario:\nQuery all jobs run in your cluster in the past week with the command sacct --starttime=$(date -d \u0026#34;7 days ago\u0026#34; +%Y-%m-%d) -- format=\u0026#34;JobID,User,JobName,Partition,Account,AllocCPUS,State,ExitCode\u0026#34; The example output in Figure-5 shows the unique JobID of each job submission, which user submitted it, which partition (queue) the job was submitted from, how many CPUs were used to run that job, and the status of that job. Analyze that data to identify broader trends in your cluster. Note that most submitted jobs were completed, yet job 1236 failed, job 1238 got cancelled, and job 1240 is a large running job with 16 allocated CPUs. Query utilization over a single month with the command sreport cluster AccountUtilizationByUser start=2025-04-01 end=2025-04-30 -t percent format=\u0026#34;Accounts,Login,Proper,Used\u0026#34; The example output in Figure-6 shows cluster utilization by project and user. Analyze these trends to identify adjustments to your allocation strategy across users and accounts. Identify whether it is fair that project\\_a01 and particularly user1 are hoarding 42% of resources over the month. Query top users over the prior month with the command sreport user topusage start=2025-03-01 end=2025-03-31 The example output in Figure-7 lists the top users by CPU minutes in the cluster. Note that user1 continues to be the largest user in the prior month. Figure 5 – Weekly Jobs report using sacct command. Figure 6 – Monthly cluster utilization report using sreport command. Figure 7 – Monthly top users report using sreport command. Use Case 4: Identify Job Issues Individual users want usage report summaries to identify and remediate job failures. One scenario:\nUser checks their failed jobs in the past week with the command sacct -u username --starttime=$(date -d \u0026#34;7 days ago\u0026#34; +%Y-%m-%d) --format=\u0026#34;JobID,JobName,State,ExitCode,Start,End,MaxRSS,MaxVMSize,Comment\u0026#34; The example output in Figure-8 helps the user identify that two jobs have failed due to high memory usage, and the third job succeeded as it was more memory efficient (2800MB used). The user reviews the job scripts for both cnn_test and bert_run and identifies the root cause – the scripts are not requesting enough memory. The user can then evaluate whether they want to modify the scripts to request sufficient memory and re-submit those two jobs. Figure 8 – Weekly job failures report for a particular user using sacctcommand. For more accounting use cases see SchedMD documentation for the sacctmgr, sacct, and sreport commands.\nPricing Enabling accounting will incur two additional charges – an hourly accounting usage fee that varies by cluster controller size chosen, and an accounting storage fee that is billed in per GB-month increments. The accounting usage fee is billed while accounting is enabled on your cluster, and the accounting storage fee scales based on the number of accounting records stored (configurable via the Default Purge Time parameter explained in the documentation). More details are on the PCS pricing page.\nConclusion In this post, we showed how you can leverage the managed accounting feature in AWS Parallel Computing Service to monitor cluster usage on your cluster. Get started today by visiting the AWS PCS management console. Let us know what you think!\nTAGS: HPC\nRamin Torabi\nRamin Torabi is a Senior Specialist HPC Solutions Architect at AWS. He enables customers in central Europe architecting their solutions around HPC. He has more than 15 years of experience with HPC and CAE especially in the Automotive, Aerospace, and other manufacturing industries. Ramin received a \u0026ldquo;Dr. rer. nat.\u0026rdquo; (PhD) in theoretical nuclear structure physics from the Technical University of Darmstadt in 2009. Nick Ihli\nNick Ihli is the Director of Solutions Engineering at SchedMD. He\u0026rsquo;s been focused on HPC schedulers for 20 years and specifically has expertise with running Slurm in the cloud. He loves helping customers become more efficient with Slurm in HPC and AI environments. When he\u0026rsquo;s not talking Slurm, you can find him coaching youth on the football field or basketball court. Tarun Mathur\nTarun Mathur is a Product Manager covering HPC and scientific computing at AWS. His goal is for customers running HPC workloads to have a smooth orchestration experience on AWS. Outside work, he enjoys Brazilian jiu-jitsu, mountaineering, and searching for the best rooftop bar. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Simplify AWS AppSync Events integration with Powertools for AWS Lambda by Ana Falcao and Leandro Cavalcante Damascena | on 14 MAY 2025 | AWS AppSync, AWS Lambda, Developer Tools, Technical How-to | Permalink | Share\nReal-time capabilities have become essential in modern applications, where users expect immediate updates and interactive experiences. Whether you’re building chat applications, live dashboards, gaming leaderboards, or IoT systems, AWS AppSync Events enables these real-time features through WebSocket APIs, allowing you to build scalable and performant real-time applications, without worrying about scale or connection management.\nPowertools for AWS Lambda is a developer toolkit that includes observability, batch processing, AWS Systems Manager Store Parameter Store integration, idempotency, feature flags, Amazon CloudWatch Metrics, structured logging, and more. Powertools for AWS now supports AppSync Events through the new AppSyncEventsResolver, available in Python, TypeScript, and .NET. This new feature enhances the development experience with capabilities designed to help you focus on your business logic. The AppSyncEventsResolver provides a simple and consistent interface for processing events, with built-in support for common patterns such as filtering, transforming, and routing events.\nIn this post, you will see examples in TypeScript, but you can use the same functionality in Python and .NET functions using Powertools for AWS (Python) and Powertools for AWS (.NET).\nFigure 1 Real-time event handling architecture using AWS AppSync, Lambda, and Powertools. In this post, you’ll learn how to:\nSet up event handlers using the AppSyncEventsResolver Implement different event processing patterns for optimal performance Use pattern-based routing to organize your event handlers Leverage built-in features for common use cases Getting Started The AppSyncEventsResolver provides a simple, declarative way to handle AppSync Events in your AWS Lambda function. The event resolver allows you to listen for PUBLISH and SUBSCRIBE events. PUBLISH events occur when clients send messages to a channel, while SUBSCRIBE events happen when clients attempt to subscribe to a channel. You can register handlers for different namespaces and channels to manage your event-driven communications.\nLet’s explore how to get started and the core features that will enhance your development experience. Here’s a basic example of how to set up the AppSyncEventsResolver:\nimport { AppSyncEventsResolver, UnauthorizedException, } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; // Types for our message handling type ChatMessage = { userId: string; content: string; } // Simple authorization check const isAuthorized = (path: string, userId?: string): boolean =\u0026gt; { // check against your authorization system if (path.startsWith(\u0026#39;/chat/private\u0026#39;) \u0026amp;\u0026amp; !userId) { return false; } return true; }; // Message processing logic const processMessage = async (payload: ChatMessage) =\u0026gt; { // - Validate message content // - Store in database // - Enrich with additional data return { ...payload, timestamp: new Date().toISOString() }; }; const app = new AppSyncEventsResolver(); // Handle publish events for a specific channel app.onPublish(\u0026#39;/chat/general\u0026#39;, async (payload: ChatMessage) =\u0026gt; { // Process and return the message return processMessage(payload); }); // Handle subscription events for all channels app.onSubscribe(\u0026#39;/\\*\u0026#39;, async (info) =\u0026gt; { const { channel: { path }, request, } = info; // Perform access control checks if (!isAuthorized(path, userId)) { throw new UnauthorizedException(`not allowed to subscribe to ${path}`); } return true; }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context); The AppSyncEventsResolver class takes care of parsing the incoming event data and invoking the appropriate handler method based on the event type. Let’s break down what’s happening:\nPattern-based Routing The AppSyncEventsResolver uses an intuitive pattern-based routing system that allows you to organize your event handlers based on channel paths. You can:\nHandle specific channels (/chat/general) Use wildcards for namespaces (/chat/*) Create global catch-all handlers (/*) import { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; const app = new AppSyncEventsResolver(); // Specific channel handler app.onPublish(\u0026#39;/notifications/alerts\u0026#39;, async (payload) =\u0026gt; { // your logic here }); // Handle all channels in the notifications namespace app.onPublish(\u0026#39;/notifications/\\*\u0026#39;, async (payload) =\u0026gt; { // your logic here }); // Global catch-all for unhandled channels app.onPublish(\u0026#39;/\\*\u0026#39;, async (payload) =\u0026gt; { // your logic here }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context); The most general catch-all handler is /*, which will match any namespace and channel, while /default/* will match any channel in the default namespace. When multiple handlers match the same event, the library will call the most specific handler and ignore the less specific ones. For example, if a handler is registered for /default/channel1 and another one for /default/\\*, Powertools will call the first handler for events that match /default/channel1 and ignore the second one. This provides you control over how events are handled and to avoid unnecessary processing. If an event that does not match any handler, by default Powertools will return the events as is and log a warning. This means that the events will be passed through without any modifications. This approach is helpful for gradually adopting the library, allowing you to handle specific events with custom logic while others are processed by the default behavior.\nSubscription Handling Powertools also provides a simple way to handle subscription events. It will automatically parse the incoming event and call the appropriate handler based on the event type. By default, AppSync allows the subscription unless your Lambda handler either throws an error or explicitly rejects the request. When a subscription is rejected, AppSync will return a 4xx response to the client and prevent the subscription from being established.\nimport { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; import { Metrics, MetricUnit } from \u0026#39;@aws-lambda-powertools/metrics\u0026#39;; import type { Context } from \u0026#39;aws-lambda\u0026#39;; const metrics = new Metrics({ namespace: \u0026#39;serverlessAirline\u0026#39;, serviceName: \u0026#39;chat\u0026#39;, singleMetric: true, }); const app = new AppSyncEventsResolver(); app.onSubscribe(\u0026#39;/default/foo\u0026#39;, (event) =\u0026gt; { metrics.addDimension(\u0026#39;channel\u0026#39;, event.info.channel.path); metrics.addMetric(\u0026#39;connections\u0026#39;, MetricUnit.Count, 1); }); export const handler = async (event: unknown, context: Context) =\u0026gt; app.resolve(event, context); The library calls the appropriate handler and passes the event object as the first argument when a subscription event arrives. You can take any necessary actions based on the subscription event, such as running access control checks:\napp.onSubscribe(\u0026#39;/private/\\*\u0026#39;, async (info) =\u0026gt; { const userGroups = info.identity?.groups \u0026amp;\u0026amp; Array.isArray(info.identity?.groups) ? info.identity?.groups : []; const channelGroup = \u0026#39;premium-users\u0026#39;; if (!userGroups.includes(channelGroup)) { throw new UnauthorizedException( `Subscription requires ${channelGroup} group membership` ); } }) Subscription events follow the same matching rules and provide the same access to the full event and context. You can register catch-all handlers for any namespace or channel by using the wildcard * character, and also access the full event and context objects in your handlers.\nAccess full event and context While the resolver simplifies event handling, you still have full access to the event and context objects when needed. This is helpful in scenarios where you need additional information, such as request headers or the remaining execution time from the Lambda context, to implement custom logic.\nThe resolver passes the full event and context to each handler as the second and third arguments. This lets you access all relevant information without changing your existing code\nimport { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; import { Logger } from \u0026#39;@aws-lambda-powertools/logger\u0026#39;; const logger = new Logger({ logLeveL: \u0026#39;INFO\u0026#39;, serviceName: \u0026#39;serverlessAirline\u0026#39; }); const app = new AppSyncEventsResolver({ logger}); app.onPublish(\u0026#39;/orders/process\u0026#39;, async (payload, event, context) =\u0026gt; { // Access request headers const { headers } = event.request; // Access Lambda context const { getRemainingTimeInMillis } = context; logger.info(\u0026#39;Processing event details\u0026#39;, { headers, remainingTime: getRemainingTimeInMillis() }); return payload; }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context);: Error handling The AppSyncEventsResolver offers built-in error handling that prevents Lambda function failures while ensuring errors are properly communicated to AppSync, which then propagates them to the clients. When an error occurs in your handler, instead of failing the entire Lambda invocation, the resolver captures the error and includes it in the response payload for the specific event that failed.\nThis approach ensures the Lambda function continues execution while providing properly formatted error messages to AppSync. When processing multiple events, if one event fails, the others continue processing normally. This is particularly useful in parallel processing scenarios where you want to ensure that an error in one event doesn’t affect the processing of other events.\nimport { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; const app = new AppSyncEventsResolver(); app.onPublish(\u0026#39;/messages\u0026#39;, async (payload) =\u0026gt; { // If message contains \u0026#34;error\u0026#34;, throw an exception if (payload.message === \u0026#34;error\u0026#34;) { throw new Error(\u0026#34;Invalid message\u0026#34;); } return payload; }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context); // When processing this event: // { // \u0026#34;id\u0026#34;: \u0026#34;123\u0026#34;, // \u0026#34;payload\u0026#34;: { // \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34; // } // } // The resolver will return: // { // \u0026#34;id\u0026#34;: \u0026#34;123\u0026#34;, // \u0026#34;error\u0026#34;: \u0026#34;Error - Invalid message\u0026#34; // } Advanced Patterns and Best Practices The AppSyncEventsResolver has additional advanced features that help you build robust and maintainable real-time applications. Let’s explore these capabilities and how to use them effectively.\nOn publish processing By default, we call your route handler once per message. This allows you to focus on your business logic and avoid writing boilerplate code while Powertools handles message iteration and converts the event and response format. You only need to return the value you want to use as payload, or throw an error for that message. The library will then correlate the payload with the correct event id.\nimport { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; import { Metrics, MetricUnit } from \u0026#39;@aws-lambda-powertools/metrics\u0026#39;; type SensorReading = { deviceId: string; temperature: number; humidity: number; timestamp: string; } const app = new AppSyncEventsResolver(); const metrics = new Metrics({ namespace: \u0026#39;SensorReadings\u0026#39; }); app.onPublish(\u0026#39;/sensors/readings\u0026#39;, async (payload: SensorReading) =\u0026gt; { // Process each sensor reading independently if (payload.temperature \u0026gt; 100) { metrics.addDimension(\u0026#39;alertType\u0026#39;, \u0026#39;highTemperature\u0026#39;); metrics.addMetric(\u0026#39;HighTemperature\u0026#39;, MetricUnit.Count, 1); throw new Error(\u0026#39;Temperature reading too high\u0026#39;); } // Enrich the payload with processing timestamp return { ...payload, processed: true, processedAt: new Date().toISOString() }; }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context); This pattern simplifies development by letting you write only the logic for a single event, Powertools handles the rest automatically.\nAggregate Processing The aggregate mode lets you to process multiple events as a single batch, rather than handling them individually. This is particularly useful when you want to optimize resource usage, such as sending multiple queries to a database in a single operation, or to analyze multiple events together before processing them. While both modes give you full control over event processing, aggregate mode provides access to the entire list of events at once.\nTo achieve this, you can set the aggregate option to true. When using this mode, the resolver sends the entire list of events to your handler in a single call, letting you process them as a batch.\nimport { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; const app = new AppSyncEventsResolver(); app.onPublish(\u0026#39;/default/*\u0026#39;, async (events) =\u0026gt; { const results = []; for (const event of events) { try { results.push(await handleDefaultNamespaceCatchAll(event)); } catch (error) { results.push({ error: { errorType: \u0026#39;Error\u0026#39;, message: error.message, }, id: event.id, }); } } return results; }, { aggregate: true, }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context); Note that the aggregate option is only available for publish events, and that when using this option, you are responsible for handling the events and returning the appropriate response. Powertools will still take care of routing the events to the correct handler, but you have full control over how the events are processed.\nEvent Filtering To filter out an event, throw an error in the channel handler. If a handler throws an error for a specific event, the library catches it and adds an error object to the response list at the same index. This signals that the corresponding message should be dropped. This allows you to either silently filter events or provide meaningful error feedback to your subscribers.\nimport { AppSyncEventsResolver } from \u0026#39;@aws-lambda-powertools/event-handler/appsync-events\u0026#39;; app.onPublish(\u0026#39;/moderation/\\*\u0026#39;, async (payload) =\u0026gt; { // Filter out inappropriate content if (await containsInappropriateContent(payload)) { throw new CustomError(\u0026#39;Content violates guidelines\u0026#39;); } // Process valid content return await processContent(payload); }); export const handler = async (event, context) =\u0026gt; app.resolve(event, context); Conclusion The AppSyncEventsResolver in Powertools for AWS enhances your development experience with AppSync Events by providing a simple and consistent interface for processing real-time events. By reducing boilerplate code and offering built-in support for common patterns, you can focus on your business logic rather than infrastructure code.\nTo learn more:\nExplore the Powertools for AWS documentation for detailed information. Check out the AppSync Events documentation to learn more about the service. Visit our GitHub repository to contribute or report issues. We’re excited to see what you build with these new capabilities. Share your feedback and let us know how you’re using AppSyncEventsResolver in your applications!\nTAGS: AppSync, AWS Lambda\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Nhat Kim Ngan\nPhone Number: +84 333 982 942\nEmail: ngannnkse182088@fpt.edu.vn\nUniversity: Ho Chi Minh FPT University\nMajor: Software Engineering\nClass: OJT202\nInternship Company: Amazon Web Services Vietnam LLC\nInternship Position: FCJ Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services and how to use the AWS Console \u0026amp; CLI. Tasks to be Carried Out This Week Day Task Start Date Completion Date Reference Material 2 Make friends with FCJ members.\nPay attention to rules for job training.\nPractice:\n+ Create AWS Free Tier account 08/09/2025 09/09/2025 — 3 Learn AWS \u0026amp; AWS Services:\n+ EC2\n+ Lambda\n+ SQS\n+ SNS\n+ CLI \u0026amp; SDKs 09/09/2025 10/09/2025 — 4 Learn AWS ECS, AWS EKS, VPC, CloudFormation.\nLearn Cost Management.\nPractice:\n+ Create EC2 instance\n+ Create Lambda function\n+ Use AWS CLI 10/09/2025 11/09/2025 — 5 Learn VPC Basics and S3.\nPractice:\n+ Create Security Group\n+ Create Internet Gateway\n+ Create Subnet\n+ Create Route Table 11/09/2025 12/09/2025 — 6 Practice:\n+ Launch S3 Bucket 12/09/2025 13/09/2025 — Week 1 Achievements 1. AWS Foundations Understood the core AWS service groups: Compute, Storage, Networking, Database. Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console and learned how to navigate services. 2. AWS CLI Skills Performed basic operations using AWS CLI: Check account \u0026amp; configuration details Retrieve list of regions View EC2 information Create/manage Key Pairs Check running services "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives Learn about Amazon RDS, AWS S3, AWS EC2 Auto Scaling, AWS DynamoDB, and Amazon Lightsail. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Learned AWS DynamoDB, AWS RDS, and Amazon Aurora. - Practice: + Created AWS CloudFront and AWS S3. 14/09/2025 15/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX/ 3 Practice: + Created AWS RDS. + Created DynamoDB using Python and AWS CLI. 15/09/2025 16/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 4 Translated blog articles. 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learned fundamentals of EC2 Auto Scaling and Amazon Lightsail.\n- Practice: + Created a database in AWS RDS.\n+ Used Amazon Lightsail. 19/09/2025 20/09/2025 https://000005.awsstudygroup.com/ https://000045.awsstudygroup.com/ 6 Practice: + Created RDS, VPC, and EC2 with AWS EC2 Auto Scaling. 20/09/2025 21/09/2025 https://000006.awsstudygroup.com/ Week 2 Achievements 1. Static Website Hosting with Amazon S3 Learned how to create an S3 bucket, upload files, and manage access. Enabled Static Website Hosting on an S3 bucket. Configured public access so the website can be accessed over the Internet. Set up an appropriate Bucket Policy to allow public read-only access. 2. Database Essentials with Amazon RDS Understand that RDS supports many popular engines such as MySQL, PostgreSQL, Oracle, and SQL Server. Learned how to connect an EC2 instance to an RDS database. Identified key connection information: endpoint, port, username, and password. Created and configured VPC, subnets, and security groups to secure an RDS instance. 3. Scaling Applications with EC2 Auto Scaling Understood that Auto Scaling automatically increases or decreases EC2 instances based on demand. Learned core components of Auto Scaling: Launch Configuration / Launch Template Auto Scaling Group (ASG) Scaling Policies: Dynamic Scaling and Scheduled Scaling Health Checks Used Amazon CloudWatch to monitor metrics such as CPU utilization, network traffic, and request count to support scaling decisions. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives Understand AWS security fundamentals. Learn key security-related services such as AWS KMS, Amazon Macie, and AWS Certificate Manager. Understand how to protect AWS infrastructure. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Learned data protection using AWS built-in capabilities (Amazon S3, Amazon EBS, Amazon DynamoDB) and AWS security services (AWS KMS, Amazon Macie, AWS Certificate Manager). 22/09/2025 23/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 3 Learned how to protect networks and applications using AWS infrastructure security (Security Groups, ELB, AWS Regions) and AWS protection services (AWS Shield, AWS WAF). 23/09/2025 24/09/2025 Same as above 4 Studied threat detection and incident response using Amazon Inspector, Amazon GuardDuty, Amazon Detective, and AWS Security Hub. 24/09/2025 25/09/2025 Same as above 5 Learned how to prevent unauthorized access using additional services such as IAM Identity Center, AWS Secrets Manager, and AWS Systems Manager. 25/09/2025 26/09/2025 Same as above 6 Practice: + Implemented Identity Federation using AWS Single Sign-On (AWS IAM Identity Center). 26/09/2025 29/09/2025 https://000012.awsstudygroup.com/ Week 3 Achievements 1. Data Protection and Governance Understood: Encryption \u0026amp; key management using AWS KMS Sensitive data discovery using Amazon Macie Certificate lifecycle management with AWS Certificate Manager Built-in security features of S3, EBS, and DynamoDB 2. Network and Application Security Learned layered security concepts: Security Groups Elastic Load Balancers (ELB) Multi-Region protection strategies Understood how AWS Shield and AWS WAF mitigate DDoS and web attacks. 3. Threat Detection \u0026amp; Incident Response Understood the roles of: Amazon Inspector (vulnerability scanning) Amazon GuardDuty (threat detection) Amazon Detective (investigation \u0026amp; analysis) AWS Security Hub (centralized security posture) 4. Preventing Unauthorized Access Learned fundamentals of: IAM Identity Center (SSO) AWS Secrets Manager AWS Systems Manager "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Practice creating, managing, and deploying AWS resources using various tools (Console, CLI, SDK, Elastic Beanstalk, etc.). Build a prototype web application using Lambda, S3, DynamoDB, and API Gateway. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Practice: Create a table in AWS DynamoDB using the AWS CLI or Python SDK (boto3) with Access Keys. 28/09/2025 29/09/2025 3 Practice: Build a Book Store application using AWS Lambda, S3, and DynamoDB. 30/09/2025 01/10/2025 https://000078.awsstudygroup.com/ 4 Learn JSON \u0026amp; Document data models. 01/10/2025 02/10/2025 5 Practice:\n+ Deploy and verify AWS resources using an AWS CloudFormation template.\n+ Use AWS Tools for Eclipse to deploy a Java application to an Elastic Beanstalk environment.\n+ Install and configure the Elastic Beanstalk CLI.\n+ Use the EB CLI to deploy updates to an existing environment.\n+ Use the AWS SDK to query and modify AWS resources programmatically. 03/10/2025 04/10/2025 https://000050.awsstudygroup.com/ 6 Practice: Build a frontend web application that interacts with the database through Lambda and API Gateway. 04/10/2025 05/10/2025 https://000079.awsstudygroup.com/ Week 4 Achievements 1. Working with AWS DynamoDB Created and managed DynamoDB tables using AWS CLI and Python SDK (boto3). Defined Primary Keys, Sort Keys, and configured Read/Write Capacity Modes. 2. Built a Book Store Application (Lambda + S3 + DynamoDB) Created Lambda functions to perform CRUD operations on book data. Integrated API Gateway with Lambda to build a RESTful backend. Stored book images and static web assets in Amazon S3. 3. Understood JSON \u0026amp; Document Data Models Learned how DynamoDB stores semi-structured data. Queried and updated data using JSON-based structures. 4. Deployed a Java Application with Elastic Beanstalk Deployed infrastructure using AWS Console and CloudFormation templates. Installed and configured the Elastic Beanstalk CLI. Deployed updates to an existing environment. Used the AWS SDK for Java to interact programmatically with AWS. 5. Built a Frontend Web App with Serverless Backend Created a simple UI for adding, editing, and deleting items in DynamoDB. Understood the workflow: Frontend → API Gateway → Lambda → DynamoDB. Learned basic approaches to securing serverless APIs. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives Explore and practice authentication and storage using AWS Amplify. Understand and deploy serverless applications using AWS SAM (Serverless Application Model). Learn and implement user authentication using Amazon Cognito. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Learn AWS Amplify. 05/10/2025 06/10/2025 https://aws.amazon.com/vi/amplify/ 3 Practice: Use Amplify Authentication and Amplify Storage for serverless applications. 06/10/2025 09/10/2025 https://000134.awsstudygroup.com/ 4 Learn AWS SAM: + SAM templates and SAM CLI + SAM Accelerate and SAM CLI integration. 09/10/2025 10/10/2025 https://aws.amazon.com/vi/serverless/sam/ 5 Practice: + Install SAM CLI + Deploy front-end and Lambda functions + Configure API Gateway and test API via Postman. 10/10/2025 11/10/2025 https://000080.awsstudygroup.com/ 6 Learn Amazon Cognito and Practice: Implement authentication using Cognito. 11/10/2025 13/10/2025 https://000081.awsstudygroup.com/ Week 5 Achievements 1. Worked with AWS Amplify Learned Amplify architecture and how it integrates with serverless systems. Implemented Amplify Authentication for user registration and login flows. Used Amplify Storage to manage file uploads and access through Amazon S3. Managed Amplify environments using CLI and integrated with front-end applications. 2. Studied and Applied AWS SAM (Serverless Application Model) Learned structure of SAM templates; understood CloudFormation integration. Installed and configured SAM CLI for local development and deployment. Practiced deploying serverless applications using SAM Accelerate. Deployed Lambda functions and tested API endpoints via API Gateway using Postman. Understood workflow: SAM → Lambda → API Gateway → CloudFormation. 3. Learned and Practiced Amazon Cognito Understood Cognito User Pools (authentication) and Identity Pools (authorization + AWS service access). Implemented user authentication workflows using Cognito. Integrated Cognito with AWS Amplify for seamless front-end authentication. Successfully tested sign-up, sign-in, confirmation, and token verification flows. 4. Combined AWS Amplify, SAM, and Cognito Built a fully functional serverless application supporting authentication, API interaction, and cloud deployment. Strengthened overall understanding of serverless architecture, preparing for advanced cloud-native development tasks. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives Understand and practice AWS Backup, including creating Backup Vaults, Backup Plans, and deploying backup configurations using AWS CloudFormation. Learn how AWS WAF (Web Application Firewall) and AWS PrivateLink work, including key components such as ACLs, Rules, Rule Groups, VPC Endpoint Services, and Network Load Balancers. Study AWS KMS (Key Management Service) — covering symmetric and asymmetric key management and their role in data encryption. Gain a solid understanding of Containerization with Docker, including how to build and deploy applications using Docker Images and Docker Compose. Strengthen skills in Infrastructure as Code (IaC) and container-based deployment for secure and scalable cloud environments. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS Backup: Backup vault, Backup plan, Using CloudFormation to create backup plan 13/10/2025 15/10/2025 https://000013.awsstudygroup.com/ 3 Practice: Create backup plan, on-demand backup, backup vaults 13/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn AWS WAF and AWS PrivateLink: ACL, Rules, Rule Groups, VPC Endpoint Service, VPC Endpoint, Network Load Balancer 15/10/2025 17/10/2025 https://000026.awsstudygroup.com/ https://000111.awsstudygroup.com/ 5 Learn AWS KMS: Symmetric Key, Asymmetric Key; Practice: Create ACLs, rules and rule groups 17/10/2025 19/10/2025 https://000033.awsstudygroup.com/ 6 Learn Containerization with Docker: What is Docker, Deploy using Docker Image, Deploy with Docker Compose and push image 19/10/2025 21/10/2025 https://000015.awsstudygroup.com/ Week 6 Achievements Learned and practiced AWS Backup, created Backup Vaults, Backup Plans, and On-demand Backups via AWS Console and CloudFormation. Configured and tested AWS WAF, created ACLs, Rules, and Rule Groups to protect web applications. Practiced AWS PrivateLink, implemented VPC Endpoint Services and VPC Endpoints with Network Load Balancer. Used AWS KMS, created and managed both Symmetric and Asymmetric Keys for secure data encryption. Built and deployed containerized applications with Docker, including Docker Images, containers, Docker Compose, and pushed images to Docker Hub. Strengthened understanding of Cloud Security, Encryption, and Containerization, preparing for real-world AWS deployments. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives Learn the AWS services used to optimize performance. Understand necessary services for performance optimization: ECS, EKS, CodePipeline, Storage Gateway. Learn about Docker, Kubernetes, and the relationship between Docker and Kubernetes. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS EKS: Control Plane (AWS managed), Worker Nodes (user-managed), Components: Cluster, Node Group, Pod, Deployment, Service 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/1-introduce/ 3 Practice EKS: Create network (VPC, subnets, Internet Gateway), Auth for control plane, Create EKS cluster, Install addons (vpc-cni, kube-proxy), Auth for worker node, Create worker node, Install addon coredns, Test Nginx deployment 21/10/2025 22/10/2025 https://000126.awsstudygroup.com/1-introduce/ 4 Learn AWS ECS: Cluster, Task Definition, Task, Service, Container Agent, ECS Launch Types, Networking in ECS, Integrate with other services, ECS Auto Scaling 22/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn AWS Storage Gateway and Practice ECS: Create ECS cluster, Configure Docker image, Create Task Definition, Register namespace in Cloud Map 23/10/2025 25/10/2025 https://000016.awsstudygroup.com/1-introduction/ 6 Learn AWS CodePipeline: Source stage (GitHub, CodeCommit, S3), Build stage (CodeBuild), Deploy stage (CodeDeploy) 25/10/2025 27/10/2025 https://000017.awsstudygroup.com/ Week 7 Achievements 1. AWS EKS Understand EKS architecture: Control Plane (AWS-managed) and Worker Nodes (self-managed). Learned definitions: Cluster, Node Group, Pod, Deployment, Service. Connected kubectl to EKS Cluster and managed workloads. 2. Practice on EKS Created VPC, Subnets, Internet Gateway, Route Table. Provided authentication for EKS Control Plane (IAM Role). Created EKS Cluster, installed addons: vpc-cni, kube-proxy. Created Node Group for Worker Nodes, installed addon coredns. Deployed Nginx successfully on EKS, accessible via LoadBalancer. 3. AWS ECS Understood ECS Cluster, Task Definition, Service, Container Agent. Differentiated ECS Launch Types: EC2 vs Fargate. Learned ECS networking modes: bridge, host, awsvpc. Integrated ECS with CloudWatch, Load Balancer, Auto Scaling. 4. Practice ECS Created ECS Cluster (Fargate), built and pushed Docker images to ECR. Created Task Definition and Service to run containers. Registered namespace in Cloud Map for service discovery. Learned AWS Storage Gateway basics and hybrid storage model. 5. AWS CodePipeline Learned CI/CD pipeline stages: Source → Build → Deploy. Integrated CodePipeline with GitHub, CodeBuild, CodeDeploy. Automated build and deployment of Spring Boot or React applications to EC2/S3. Wrote template files: buildspec.yml and appspec.yml. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Understand the fundamentals and use cases of AWS Step Functions, including its 7 core state types and how to orchestrate complex workflows. Gain hands-on experience creating and testing workflows in AWS Cloud9, focusing on task orchestration and error handling. Learn the key features and deployment steps of Amazon FSx, including its different variants (Windows File Server, Lustre, NetApp ONTAP, OpenZFS). Practice configuring an FSx file system integrated with AWS Managed Microsoft AD, ensuring proper setup of networking, authentication, and file sharing. Explore AWS X-Ray to understand how to trace and visualize requests across distributed applications for performance optimization and debugging. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS Step Functions: + 7 states: Task, Choice, Fail/Success, Pass, Wait, Parallel, Map + Use cases of AWS Step Functions + Benefits of AWS Step Functions 26/10/2025 27/10/2025 https://000047.awsstudygroup.com/1-intro/ 3 Practice: + Create Cloud9 environment + Create sample services + Initialize workflow + Implement error handling 27/10/2025 28/10/2025 https://000047.awsstudygroup.com/1-intro/ 4 Learn Amazon FSx: + FSx for Windows File Server + FSx for Lustre + FSx for NetApp ONTAP + FSx for OpenZFS 28/10/2025 29/10/2025 https://000025.awsstudygroup.com/ 5 Practice: + Configure file system details + Choose existing VPC + Choose AWS Managed Microsoft AD (Provide DNS, Service Account username/password) + Define Windows file share name + Review and create 29/10/2025 30/10/2025 https://000025.awsstudygroup.com/ 6 Learn AWS X-Ray: + Trace + Segment + Subsegment + Annotation / Metadata + Service Map 30/10/2025 31/10/2025 https://000140.awsstudygroup.com/ Week 8 Achievements Learned and explained the 7 states of AWS Step Functions, including their role in workflow automation and state transitions. Practiced building a sample workflow in Cloud9 with Step Functions, including error handling and execution validation. Gained understanding of different Amazon FSx options and their ideal use cases for Windows workloads, HPC, and enterprise storage. Completed hands-on setup of FSx for Windows File Server, including file system configuration, VPC/AD integration, and file share creation. Learned how AWS X-Ray collects traces, segments, subsegments, and visualized a Service Map to identify performance bottlenecks and errors. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives Understand and apply AWS AppSync for building GraphQL APIs with various data sources. Learn and configure AWS EBS Data Lifecycle Manager to automate snapshots and backups. Explore AWS GuardDuty for intelligent threat detection using ML and behavior analytics. Study AWS Macie to identify and protect sensitive data in S3 buckets using machine learning. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS AppSync: + GraphQL APIs: Query, Mutation, Subscription + Data Sources: DynamoDB, RDS/Aurora, AWS Lambda, HTTP Endpoints, OpenSearch + Authentication and Authorization: AWS IAM, API Keys, Cognito User Pools, OpenID Connect + Realtime Subscriptions, Caching, Offline support Practice: + Create GraphQL API + Define schema and attach data source + Configure request/response mapping templates 02/11/2025 03/11/2025 https://000086.awsstudygroup.com/1-introduction/ 3 Learn AWS EBS Data Lifecycle Manager: + Automate backup and recovery + Reduce storage costs + Ensure compliance and data protection + Understand Lifecycle Policies: EBS Snapshot Policy, EBS-backed AMI Policy, Cross-region/Cross-account Copy Policy 03/11/2025 05/11/2025 https://000088.awsstudygroup.com/ 4 Practice: + Launch EC2 instance and configure lifecycle policies + Define which resources to back up, schedule, and retention Learn AWS GuardDuty: + ML-based threat detection and behavioral analysis 03/11/2025 06/11/2025 https://000098.awsstudygroup.com/ 5 Learn AWS Macie: + Key Features: Data Discovery, Classification, Bucket-level Visibility + How Macie works internally + Use Cases and Pricing Model 14/08/2025 15/08/2025 https://000090.awsstudygroup.com/ 6 Practice on AWS Macie: + Create and configure an S3 bucket + Enable Macie service + Create Macie jobs to scan and classify data 15/08/2025 15/08/2025 https://000090.awsstudygroup.com/ Week 9 Achievements AWS AppSync: Built a simple GraphQL API integrated with DynamoDB and Lambda, understanding schema design, resolvers, and mapping templates. AWS EBS Lifecycle Manager: Configured automated backup policies to improve reliability and cost-efficiency in EC2 environments. AWS GuardDuty: Gained knowledge of continuous threat detection using ML and anomaly-based analysis. AWS Macie: Enabled Macie for S3, ran data classification jobs, and reviewed findings on sensitive data exposure and compliance monitoring. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn Amazon RDS,AWS DynamoDB, LightSail and EC2 Auto Scaling Week 3: Learn security\nWeek 4: Learn AWS DynamoDB and Relational Database Services\nWeek 5: authentication and storage\nWeek 6: Data encrytion\nWeek 7: optimized performance\nWeek 8: setup of networking, authentication, and file sharing\nWeek 9: Practice with security and operations\nWeek 10: Learn monitoring performance\nWeek 11: Learn Redis and more AWS Security Week 12: Learn QuickSight, Athena, and data lake\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: Kick-off AWS First Cloud Journey Workforce – OJT FALL 2025 Event Objectives Officially launch the AWS First Cloud Journey (FCJ) program for the Fall 2025 cohort. Provide a comprehensive roadmap of the program, setting expectations for real-world projects and On-the-Job Training (OJT). Outline career orientations in high-demand fields: Cloud Computing, AI/GenAI, and DevOps. Inspire interns by connecting them with mentors, successful alumni, and the broader AWS Vietnam community. Time \u0026amp; Venue Time: 08:30 – 12:00, Saturday, September 06, 2025\nVenue: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Program Leaders \u0026amp; Educators\nMr. Nguyen Tran Phuoc Bao – Head of Corporate Relations, FPT University Mr. Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Industry Experts \u0026amp; Alumni Panel\nDo Huy Thang – DevOps Lead, VNG Danh Hoang Hieu Nghi – GenAI Engineer, Renova Bui Ho Linh Nhi – AI Engineer, SoftwareOne Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights 1. The FCJ Vision \u0026amp; Impact Legacy: Since 2021, FCJ has trained over 2,000 students, creating a pipeline of 150+ alumni currently working at top tech companies. Mission: To build a high-quality generation of AWS Builders for Vietnam, capable of competing globally. Ecosystem: The program is deeply connected to the AWS Study Group (47,000+ members), providing a massive support network for new joiners. 2. Industry Insights \u0026amp; Career Accelerators Rapid Growth: The sharing session highlighted how Alumni transitioned from Interns to roles like Principal Cloud Engineer or GenAI Engineer in just 1–3 years. Key Trends: The shift from traditional SysAdmin to modern roles requires a mix of Cloud Native, Infrastructure as Code (IaC), and Generative AI skills. Recruitment Reality: Employers are looking for \u0026ldquo;Builders\u0026rdquo; who can demonstrate practical project experience, not just certificate holders. 3. The \u0026ldquo;Builder\u0026rdquo; Mentality Core Advice: \u0026ldquo;Learn fast – build real projects – ask early – share more.\u0026rdquo; Proactivity: Success in the OJT program relies 80% on the intern\u0026rsquo;s proactivity in seeking mentorship and solving problems independently before asking. Key Takeaways Mindset \u0026amp; Culture Contribution Culture: Knowledge grows when shared. Active participation in the community is the fastest way to learn. Fail Fast, Learn Faster: Don\u0026rsquo;t be afraid to break things in the lab environment. It\u0026rsquo;s better to fail during OJT than in a production environment. Career Strategy T-Shaped Skills: Build a broad foundation in AWS services (Compute, Network, Storage) but develop deep expertise in one domain (e.g., Serverless or GenAI). Network is Net Worth: The relationships built with mentors and peers in this program are future professional connections. Skill Orientation Focus Areas: The current market demands proficiency in Serverless (Lambda, API Gateway), Terraform/CDK, and CI/CD pipelines. Soft Skills: Communication and documentation are just as critical as coding. Applying to Work Define Roadmap: Set a clear goal for the 4-month internship: Achieve the AWS Solutions Architect Associate certification and complete one end-to-end capstone project. Portfolio Building: Commit to documenting every weekly task and project milestone on GitHub and personal blogs (like this Hugo site). Mentorship Engagement: Schedule bi-weekly checkpoints with the assigned mentor to review code quality and discuss architectural decisions for the Student Management System project. Event Experience The Kick-off event was more than just an orientation; it was a motivational booster.\nInspiration: Hearing specific stories from alumni like Nguyen Dong Thanh Hiep (Principal Cloud Engineer) proved that a steep career trajectory is possible with the right focus. Atmosphere: The energy at the AWS Office was palpable. Being surrounded by 300+ like-minded peers created a sense of belonging to a \u0026ldquo;technical elite\u0026rdquo; squad. Direct Connection: I had the opportunity to listen directly to Mr. Nguyễn Gia Hưng, and his vision for Vietnam’s technology workforce truly motivated me to strive harder. Event Photos "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Serverless Student Management System Cloud-based serverless student management system for students and small businesses 1. Executive Summary Serverless Student Management Platform is a cloud-based student management platform designed for educational institutions and small businesses to enhance the management, analysis, and interaction of student data. The platform supports up to 50-100 students initially, with the ability to flexibly scale up to 500-1000 students without major infrastructure changes, using AWS Serverless services such as Lambda, DynamoDB, AppSync, and EventBridge.\nUsers can access it via a friendly web interface (supporting real-time chat for assignments and comments) combined with APIs for real-time data transmission, ensuring high interactivity. The platform leverages AWS to provide centralized management (secure data storage), predictive analytics (via Personalize for score rankings and recommendations), reminder notifications (via SES), and system monitoring (via CloudWatch), while optimizing costs by charging only for actual usage.\n2. Problem Statement Current Issues\nCurrent student management systems require manual data entry, making it difficult to manage multiple classes. There is no centralized system for data or real-time analytics, and third-party platforms are often expensive and overly complex. Lack of real-time communication features leads to delayed communication, absence of incentive mechanisms like diligence ranking, and manual exam event processing.\nSolution\nThe platform uses Amazon API Gateway to handle REST requests, AWS Lambda for business logic processing, Amazon DynamoDB for storing student data and scores, and AWS AppSync with GraphQL for real-time communication. AWS Amplify with React/Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to traditional LMS systems but with lower costs, users can register new students and manage information, but this platform operates at a smaller scale and serves educational purposes. Key features include real-time management dashboard, learning trend analysis, and low operational costs.\nBenefits and Return on Investment (ROI)\nThe solution creates a foundation for IT students to develop AWS serverless skills while providing efficient management tools for teachers in instruction and assessment. The platform reduces manual reporting for each class through centralized systems, simplifies management and maintenance, and improves data reliability. Monthly costs estimated at $7-20 USD (according to AWS Pricing Calculator), totaling $21-60 USD for 3 months. All AWS resources are used within free tier and low-cost options, with no additional development costs. ROI period of 3-6 months through significant savings in manual operations time and improved teaching efficiency.\n3. Solution Architecture The system is designed following the AWS Well-Architected Framework with interconnected layers, ensuring data management capabilities, authentication, monitoring, real-time communication, ML ranking, notifications, and continuous event processing. The serverless architecture helps optimize costs and ensures automatic scalability.\nAWS Core Services Used Service Primary Function Key Benefits Amazon Route 53 DNS management and routing traffic to CloudFront Custom domain setup, health checks, geo-routing, SSL certificates integration Amazon CloudFront CDN distribution for static website and frontend resources Reduce global latency, Edge caching, HTTPS, support HTTP/2 and HTTP/3, integrate WAF/Shield, custom domain (ACM), OAC protects S3 AWS WAF Web Application Firewall protection against attacks Block malicious requests, rate limiting, IP filtering, CloudFront integration AWS Amplify Deploy and host frontend dashboard with CI/CD Easy build/deploy web apps, seamless integration with Cognito and AppSync AWS AppSync Support GraphQL API with subscriptions for realtime chat Real-time data updates via WebSockets, easy integration with DynamoDB and Lambda Amazon API Gateway Handle API requests and connect frontend-backend Support RESTful APIs, throttling and caching to improve performance Amazon Cognito Manage user authentication and authorization Support MFA, JWT tokens, easy integration with frontend AWS Lambda Execute CRUD logic without servers Auto-scale, pay-per-use, reduce operational costs Amazon DynamoDB Store NoSQL data for student information and chat messages Fast queries, auto-scale, support GSI for complex searches Amazon CloudWatch Monitor system logs and metrics Real-time monitoring, alarms to detect issues early Amazon S3 Store static files for frontend dashboard Cheap static website hosting, high durability Amazon EventBridge Route events from management web to process exams (update scores, reminders) Event-driven architecture, integrate with Lambda to automate workflows AWS Personalize Build ML models to rank students based on activity data Personalized ranking, auto-learn from data, support real-time inference AWS SES Send ranking notification emails and system updates Easy integration with Lambda, support millions of emails/month, low cost AWS CodeBuild Build and test code from GitLab repository Auto compile, package artifacts, run unit tests in pipeline AWS CodeDeploy Deploy applications to AWS services Blue/green deployment, rollback capabilities, zero-downtime deployment AWS CodePipeline Orchestrate CI/CD pipeline from source to deploy Automate the entire process, integrate with GitLab/CodeBuild/CodeDeploy, support approvals and notifications. GitLab Source control and trigger CI/CD pipeline Version control, merge requests, issue tracking, webhook integration Thiết kế thành phần Layer Thành phần chính Chức năng Edge Layer Route53, CloudFront, WAF DNS, CDN, layer security Frontend Layer Amplify, AppSync, Cognito Web interface, realtime chat, authentication Backend Layer API Gateway, Lambda, DynamoDB Logic processing, CRUD, data storage Event \u0026amp; ML Layer EventBridge, SES, Personalize Send notifications, process ML ranking Monitoring Layer CloudWatch Logs, metrics, alerts CI/CD Layer GitLab, CodePipeline, CodeBuild, CodeDeploy, S3 Build \u0026amp; deploy automation Chú thích luồng chính Luồng Mô tả (1) User access via Route53 → CloudFront → Amplify (frontend). (2) Amplify communicates with API Gateway or AppSync to send/receive data. (3.1) API Gateway → Lambda to handle logic (CRUD, events, etc.). (3.2) Cognito authenticates and returns tokens. (4) Lambda emits events via EventBridge. (4.1) EventBridge → SES sends reminder emails. (4.2) EventBridge → Lambda → Personalize for ML ranking. (5) Dev pushes code to GitLab → CodePipeline pull → CodeBuild/CodeDeploy → Amplify/Lambda. (6) CloudWatch collects logs and metrics. (7) AppSync sends realtime data (chat/notifications) to the frontend. 4. Technical Implementation Implementation Phases The project consists of 2 parts — developing a serverless student management system and building an intelligent analytics platform — each going through 4 phases:\nResearch and Architecture Design: Research AWS serverless patterns with DynamoDB and design architecture integrating realtime chat/ranking (1 month before internship). Cost Calculation and Feasibility Check: Use AWS Pricing Calculator for estimation and adjust scope accordingly (Month 1). Architecture Adjustment for Cost/Solution Optimization: Fine-tune (e.g., optimize Lambda with React SSR) to ensure efficiency (Month 2). Development, Testing, Deployment: Program React frontend, AWS services with CDK/SDK and AppSync subscriptions, then test and deploy to production (Month 2–3). Technical Requirements Student Management System: Web dashboard (CRUD students, scores, courses), real-time chat, ML ranking. Frontend React/Next.js running on Amplify Hosting, using AppSync subscriptions to receive real-time chat messages and ranking updates. Cognito authenticates and authorizes all users, including 5-10 admins/teachers (with high privileges like CRUD data) and students (with limited privileges like viewing personal scores, joining chat).\nIntelligent Analytics Platform: Practical knowledge of AWS Amplify (hosting React), Lambda (minimized due to SSR handling), AWS Personalize (ML ranking), DynamoDB (main NoSQL), AppSync (GraphQL + subscriptions), and EventBridge (event routing). Use AWS CDK/SDK for programming (e.g., EventBridge rules to Lambda for notifications). React SSR helps reduce Lambda load for fullstack web applications.\n5. Roadmap \u0026amp; Key Milestones The project is implemented over 14 weeks (from September to December 2025), divided into 6 main phases following the Agile model.\nPhase Timeline Main Objective Deliverables Success Criteria Phase 1: Foundation Setup Weeks 1–2 Set up AWS environment • AWS account setup\n• DynamoDB \u0026amp; Cognito config\n• IaC templates\n• AWS infrastructure working stably\nPhase 2: Backend Deployment Weeks 3–5 Build API and logic • Lambda functions\n• API Gateway \u0026amp; AppSync endpoints • CRUD \u0026amp; chat mutations working Phase 3: Frontend Integration Weeks 6–7 Deploy dashboard \u0026amp; chat • S3 + CloudFront hosting\n• Realtime subscriptions integration • Dashboard \u0026amp; chat accessible in real-time Phase 4: ML \u0026amp; Ranking Weeks 8-9 Integrate Personalize • Datasets import\n• Ranking API • Student ranking working Phase 5: Notification \u0026amp; Monitoring Weeks 10–11 Integrate SES \u0026amp; monitoring • Email notifications\n• CloudWatch alarms\n• Notifications sent accurately Phase 6: Testing \u0026amp; Review Weeks 12–14 Testing and finalization • End-to-end tests\n• Documentation \u0026amp; Demo • Stable system, complete demo 6. Budget Estimate You can view the cost on AWS Pricing Calculator or download the budget estimate file.\nInfrastructure Costs AWS Services:\nService Usage Description Estimated Cost / month (USD) Notes Amazon API Gateway Process ~1M API calls/month ~$1.00 – $3.50 HTTP APIs: $1.00/million calls; REST: $3.50/million; free tier 1M calls. AWS Lambda ~1M requests, 400k GB-seconds ~$0.20 – $0.50 Requests: $0.20/million; Duration: $0.0000166667/GB-second; free tier sufficient. Amazon DynamoDB ~25 GB storage, 2.5M reads/writes ~$0.25 – $1.25 Reads: $0.25/million; Writes: $1.25/million; Storage: $0.25/GB; free tier 25 GB. Amazon Cognito ~10k monthly active users ~$0.0055 – $0.015 Essentials: $0.015/MAU; free tier 10k MAUs. Amazon S3 ~5 GB storage, low requests ~$0.023 – $0.12 Storage: $0.023/GB; Requests: $0.0004/1k GET; free tier credits. Amazon CloudWatch ~5 GB logs, 10 metrics/alarms ~$0.03 – $0.50 Logs: $0.50/GB ingestion; Storage: $0.03/GB; Metrics: $0.30/metric; free tier 5 GB. AWS AppSync ~1M queries/subscriptions, realtime chat ~$0.50 – $2.00 Requests: $4.00/million; Data transfer: $0.09/GB; free tier 250k requests. AWS Personalize Train model weekly, ~10k interactions ~$0.50 – $5.00 Training: $0.25/hour; Inference: $0.00005/request; Storage: $0.05/GB. AWS SES ~10k emails/month ~$0.10 – $0.30 $0.10/1k emails; free tier 62k emails/first month. Amazon EventBridge ~10k events/month, rules for exams ~$0.10 – $0.50 $1.00/million events; free tier 100k events/month. AWS Amplify Hosting dashboard, few builds/month ~$0.00 – $1.00 Build minutes: $0.01/minute; Hosting: $0.15/GB served; free tier available. Amazon Route 53 DNS queries, hosted zones ~$0.50 – $1.00 Hosted zone: $0.50/zone/month; DNS queries: $0.40/million queries. AWS WAF Web ACL evaluation, rules ~$1.00 – $3.00 Web ACL: $1.00/month; Rules: $0.60/million requests; managed rules additional. AWS CodeBuild ~10 builds/month, 100 minutes ~$0.00 – $0.50 $0.005/build minute; free tier 100 minutes/month. AWS CodeDeploy Application deployments ~$0.00 – $0.20 EC2/on-premise: $0.02/deployment; serverless free. Amazon CloudFront CDN delivers static content (Route53 \u0026amp; Amplify/S3 connection) ~$0.10 – $1.00 First 1 TB/month free in Free Tier; $0.085/GB thereafter. AWS CodePipeline CI/CD automation (GitLab connection → CodeBuild → CodeDeploy) ~$1.00 $1.00/month for each active pipeline. Total Estimate Total cost / month (estimate) Total 3 months (estimate) Notes ~$7 – $20 / month ~$21 – $60 / 3 months Depends on actual usage (chat, ranking, emails, events); leverage free tier. Additional Route53, WAF, CI/CD costs. Notes All main services are within first year Free Tier. Can reduce further if using AWS Educate / Credits for Students. Personalize \u0026amp; SES costs based on volume; optimize with batch processing. EventBridge costs based on number of events; optimize with filter rules. 7. Risk Assessment Based on NIST Risk Management Framework, the project team identifies key risks and mitigation measures.\nRisk Code Description Level Mitigation R1 – Data Leakage Data exposure due to misconfiguration High Apply Cognito auth, IAM least privilege, DynamoDB encryption R2 – API Overload Too many requests causing slowdown Medium Throttling API Gateway/AppSync, CloudWatch alarms R3 – Lambda Cold Start Delay on invocation Medium Optimize code, Provisioned Concurrency if needed R4 – Cost Overrun Unusual usage increase (high chat, emails) Medium AWS Budgets alerts, monitor Cost Explorer R5 – Service Downtime AWS interruption Low Multi-AZ config, DynamoDB backups R6 – Realtime Latency Chat delay due to subscriptions Medium Use AppSync caching, test with high load R7 – ML Accuracy Inaccurate ranking due to poor data Medium Validate datasets, periodic retraining with Lambda R8 – Event Processing Failure Error routing exam events Medium Dead-letter queues for EventBridge, test rules Contingency Plan (Summary):\nRecovery: Use CloudFormation for fast rebuild. Communication: CloudWatch alarms send emails. Continuous Improvement: Weekly review, including chat performance, ranking, and event handling. 8. Expected Results Technical Results:\nComplete serverless student management system with full CRUD, real-time chat, ML ranking, email notifications, and exam event processing. Integration of API Gateway, Lambda, DynamoDB, Cognito, S3, Amplify, CloudWatch, AppSync, Personalize, SES, EventBridge. Response time \u0026lt;1 second, chat latency \u0026lt;2 seconds, event processing \u0026lt;5 seconds, uptime ≥99%. Actual cost ≤ $20/3 months. Learning and Training Results:\nLearners master serverless development, real-time apps, and event-driven architecture. Prepare for AWS Developer Associate certification. Develop IaC, NoSQL, Authentication, GraphQL, Personalization, Events skills. Professional \u0026amp; Presentation Results:\nReport with dashboard, chat, ranking, email, and event demo. Demo CRUD, real-time chat, notifications, and exam event processing. Deployment guide documentation. Long-term Value:\nEasy to expand to mobile app or advanced AI analytics. Platform for serverless, real-time, and event-driven training labs. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track: Migration \u0026 Modernization) Event Objectives Update on strategic Cloud, GenAI, and Modernization trends for 2025–2026 in Vietnam. Gain insights directly from senior leadership of AWS APJ and major enterprises (Techcombank, OCB, TymeX, U2U Network). Master practical best practices for migration, modernization, and applying Generative AI in the enterprise SDLC. Understand the \u0026ldquo;Security-by-design\u0026rdquo; approach in large-scale systems. Time \u0026amp; Venue Time: 09:00 – 17:00, Thursday, September 18, 2025\nVenue: 36th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Live Telecast\nEric Yeo – Country General Manager Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles – Vice President \u0026amp; General Manager Asia Pacific and Japan, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jeff Johnson – Managing Director ASEAN, AWS Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Track Migration \u0026amp; Modernization\nHung Nguyen Gia – Head of Solutions Architect, AWS Vietnam Son Do – Technical Account Manager, AWS Nguyen Van Hai – Director of Software Engineering, Techcombank Phuc Nguyen – Solutions Architect, AWS Alex Tran – AI Director, OCB Nguyen Minh Ngan – AI Specialist, OCB Nguyen Manh Tuyen – Head of Data Application, LPBank Securities Vinh Nguyen – Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang – Customer Solutions Manager, AWS Taiki Dang – Solutions Architect, AWS Key Highlights 1. AWS Cloud Strategy \u0026amp; Amazon Q Market Forecast: By 2026, 85% of large Vietnamese enterprises will have completed or be in the process of migration \u0026amp; modernization. The AI Assistant: Amazon Q Developer is positioned as the core \u0026ldquo;AI pair programmer,\u0026rdquo; supporting the entire lifecycle from coding, testing, and documentation to refactoring and debugging. 2. Real-world Migration Case Studies Techcombank: A massive scale migration case study, modernizing core banking systems on AWS to build a flexible, cost-effective platform. TymeX: Successfully applied the \u0026ldquo;Modernize while migrate\u0026rdquo; strategy, shortening their transformation timeline from 24 months down to 10 months. 3. Amazon Q Developer – The Game Changer Deep Integration: Seamlessly integrated into IDEs, AWS Console, CLI, and DevSecOps pipelines. Capabilities: Automatically generates unit tests, writes documentation, suggests code optimizations, and understands the full codebase context. Live Demo: Refactoring a legacy Java application into a Serverless architecture in just 15 minutes. 4. Modernizing VMware to AWS AWS Transform: A toolset for fast, safe, and cost-optimized migration of legacy VMware workloads. The Playbook: A clear path defined as: Migrate → Landing Zone → Modernize (to EKS, RDS, Serverless). 5. Security at Scale Security-by-Design: Embedding security controls from the development phase (Dev) through to Production. GenAI for Security: Automating detection, response, and remediation using GenAI combined with GuardDuty and Security Hub to maintain an \u0026ldquo;always secure\u0026rdquo; posture as systems scale. Key Takeaways Design Mindset Modernize while Migrate: Don\u0026rsquo;t just \u0026ldquo;lift and shift.\u0026rdquo; Look for opportunities to refactor to cloud-native services (Serverless, Managed DB) during the move to maximize value. Production Readiness: Thinking about Security, Observability, and Cost Optimization is just as important as writing the business logic. Technical Architecture GenAI in SDLC: Leveraging tools like Amazon Q is no longer optional but essential to maintain speed and code quality. Hybrid Cloud Path: Understanding the specific pathway from on-premise (VMware) to Cloud Native (EKS/Lambda). Applying to Work Adopt Amazon Q: Integrate Amazon Q Developer into the IDE (VS Code/IntelliJ) for the Student Management System project to speed up unit test generation and documentation. Serverless First: For new modules of the project, skip EC2 and design directly with AWS Lambda and API Gateway to reduce operational overhead. Refactor Legacy Code: Identify complex or \u0026ldquo;spaghetti\u0026rdquo; code sections in the current project and use Amazon Q to suggest refactoring options. Event Experience Vietnam Cloud Day 2025 provided a comprehensive vision of the industry\u0026rsquo;s future.\nStrategic Insight: Seeing how banks like Techcombank operate gave me a benchmark for what a \u0026ldquo;production-grade\u0026rdquo; system looks like. Tooling Mastery: The live demo of Amazon Q was a highlight, convincing me to adopt AI-assisted coding immediately. Networking: The atmosphere at Bitexco was vibrant, allowing me to connect with senior engineers and understand the high demand for Cloud \u0026amp; AI skills in the local market. Event Photos "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives Practice with AWS core services (VPC, EC2, EBS, Postgres) Learn AWS SageMaker for AI/ML workflow Practice AI/ML data analysis with SageMaker Learn AWS Bedrock AgentCore components Learn AWS Glue and Amazon Athena for ETL and analytics Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Practice: + Creating VPCs + Initializing EC2 instances + Creating an EBS volume + Assigning EBS volume + Installing Postgres + Mount EBS on EC2 production + Postgres backup + Mount EBS on EC2 test + Data recovery 08/11/2025 10/11/2025 https://100000.awsstudygroup.com/ 3 Learn AWS SageMaker AI: + What is SageMaker + Components + End-to-End Workflow 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 4 Practice: Use Amazon SageMaker AI to analyze data, import Excel files, choose data types, and export results including charts 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 5 Learn AWS Bedrock AgentCore: + Identity, Memory, Code Interpreter, Browser, Gateway, Observability + Common use cases: equip agents with built-in tools, deploy securely at scale, test and monitor agents 12/11/2025 14/11/2025 https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html 6 Learn AWS Glue and Amazon Athena 14/11/2025 16/11/2025 https://000040.awsstudygroup.com/ Week 10 Achievements 1. AWS Core Services Practice: Created VPCs with proper subnets, route tables, and Internet Gateways. Launched EC2 instances for production and test environments. Created and attached EBS volumes to EC2 instances. Installed and configured PostgreSQL on EC2 instances. Mounted EBS volumes on both production and test instances. Performed PostgreSQL backup and data recovery successfully. 2. AWS SageMaker AI Learning \u0026amp; Practice: Understood SageMaker components: Notebook, Training, Endpoint, Model, Pipelines. Completed end-to-end AI/ML workflow: imported Excel datasets, analyzed data, visualized charts. Learned to choose proper data types, clean data, and export results for further use. 3. AWS Bedrock AgentCore Learning: Learned core components: Identity, Memory, Code Interpreter, Browser, Gateway, Observability. Explored common use cases: equipping agents with tools, secure deployment at scale, monitoring and testing agents. 4. AWS Glue \u0026amp; Amazon Athena Learning: Practiced building ETL pipelines using AWS Glue. Learned to catalog datasets and transform data for analytics. Queried S3 data using Amazon Athena and verified results. Connected Athena queries to QuickSight dashboards for reporting and visualization. 5. AWS CLI \u0026amp; Management Console Proficiency: Configured AWS CLI with Access Key, Secret Key, and Default Region. Used CLI to retrieve regions, list services, and check configurations. Managed AWS resources via CLI and Web Console interchangeably. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives Understand and practice AWS services: ElastiCache (Redis), Certificate Manager, Inspector, Detective, Systems Manager Learn the structure, operation, and benefits of each service Practice connecting, granting permissions, deploying, and performing security checks on AWS services Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn Amazon ElastiCache - Redis: + What is Redis + Components: Clusters, ElastiCache nodes, Redis shards + How it works + Benefits of using ElastiCache 17/11/2025 18/11/2025 https://000061.awsstudygroup.com/1-introduce/ 3 Learn AWS Certificate Manager Practice: + Create cluster + Grant access to the cluster + Connect to cluster node 18/11/2025 19/11/2025 https://000061.awsstudygroup.com/ https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 4 Learn AWS Inspector: + Identify security gaps + Detect misconfigurations + Apply remediation steps 19/11/2025 20/11/2025 https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html 5 Learn AWS Detective 21/11/2025 22/11/2025 https://docs.aws.amazon.com/detective/latest/userguide/what-is-detective.html 6 Practice with AWS Systems Manager 22/11/2025 24/11/2025 https://000031.awsstudygroup.com/1-introduce/ Week 11 Achievements Completed theory and hands-on practice with ElastiCache (Redis): learned about Redis, clusters, nodes, shards, how it works, and its benefits. Practiced creating Redis clusters, granting access, and connecting to nodes. Learned and practiced AWS Certificate Manager: creating and deploying certificates on clusters. Learned and practiced AWS Inspector: identifying security gaps, misconfigurations, and remediation steps. Learned AWS Detective: detecting and investigating security events. Practiced AWS Systems Manager: centralized management and operation of AWS resources. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives Understand and practice AWS QuickSight and AWS Athena. Build a data lake from multiple data sources. Deploy a web application using Elastic Beanstalk and CDK CI/CD pipelines. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS QuickSight:\n+ Overview + Architecture + Components + SPICE 25/11/2025 26/11/2025 https://000073.awsstudygroup.com/ 3 Practice with AWS QuickSight 26/11/2025 27/11/2025 https://000073.awsstudygroup.com/ 4 Learn and Practice AWS Athena 27/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Practice building a data lake 29/11/2025 30/11/2025 https://000070.awsstudygroup.com/ 6 Practice deploying web app with Elastic Beanstalk and CDK pipeline 01/12/2025 02/12/2025 https://000113.awsstudygroup.com/ Week 12 Achievements AWS QuickSight\nLearned overview, architecture, and main components (Dashboard, Analysis, Dataset). Understood SPICE engine to accelerate queries. Practiced creating a basic dashboard and connecting datasets from S3/Athena. AWS Athena\nLearned how to query data stored on S3 using SQL. Practiced creating tables, views, and running sample queries. Data Lake\nBuilt a small data lake on S3 to consolidate multiple data sources. Performed data cleaning and transformation before analysis. Elastic Beanstalk \u0026amp; CDK\nDeployed a demo web application on Elastic Beanstalk. Configured a basic CDK pipeline for automatic deployment. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Cloud Mastery Series #1 - ​AI/ML/GenAI on AWS Event Objectives Gain a comprehensive overview of the AI/ML landscape in Vietnam and the AWS ecosystem. Understand the end-to-end Machine Learning lifecycle using Amazon SageMaker. Explore Generative AI capabilities with Amazon Bedrock, focusing on Foundation Models (FMs), RAG, and Agents. Learn practical techniques for Prompt Engineering and building AI-powered applications without deep data science expertise. Time \u0026amp; Venue Time: 08:30 – 12:00, Saturday, November 15 2025\nVenue: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Lam Tuan Kiet – Sr. DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Key Highlights 1. AWS AI/ML Services Overview (SageMaker) End-to-end Platform: Amazon SageMaker is not just for training; it covers the entire lifecycle from data preparation (Data Wrangler) to deployment and monitoring. MLOps Integration: The importance of integrating ML workflows with DevOps practices (CI/CD for ML) to ensure reproducible and scalable model deployment. Live Demo: A walkthrough of SageMaker Studio, demonstrating how to label data, train a model, and deploy an endpoint in a unified interface. 2. Generative AI with Amazon Bedrock Serverless GenAI: Bedrock allows access to high-performing Foundation Models (Claude 3, Llama 3, Titan) via a single API, removing the need to manage infrastructure. Model Selection: Guidance on choosing the right model based on cost, latency, and reasoning capability (e.g., using Claude for complex reasoning vs. Titan for simple summarization). Retrieval-Augmented Generation (RAG): The architecture for connecting LLMs to private data (Knowledge Bases) to reduce hallucinations and provide accurate, context-aware answers. 3. Advanced GenAI Features Prompt Engineering: Techniques like Chain-of-Thought and Few-shot learning to steer model behavior effectively. Bedrock Agents: Building multi-step workflows where the AI can execute API calls to perform tasks (e.g., booking a meeting, querying a database). Guardrails: Implementing safety filters to prevent the generation of harmful or inappropriate content. Key Takeaways Design Mindset Democratization of AI: You don\u0026rsquo;t need to be a PhD in Math to use AI. With services like Bedrock, software engineers can integrate powerful AI features via APIs. Context is King: A generic model is less useful than a model grounded in your specific business data. RAG is the bridge between generic intelligence and business value. Technical Architecture Managed Services vs. Self-hosted: For most use cases, using managed services (Bedrock/SageMaker) is more cost-effective and secure than hosting open-source models on EC2 instances. Safety First: AI applications must have guardrails (input/output filtering) before reaching end-users. Applying to Work Explore RAG Concepts: Research the RAG (Retrieval-Augmented Generation) architecture to understand the theoretical foundation of smart document search features. Understand Data Privacy: Learn the core principles of data protection (e.g., avoiding sending sensitive data to public models) to apply in future projects. Data Privacy: Ensure that any student data used for RAG follows strict encryption standards (KMS) and is not used to train public base models. Event Experience The workshop successfully demystified the complex world of AI/ML.\nPerspective Diversity: Hearing from a Senior DevOps Engineer (FPT) and an AI Engineer (Renova) provided a balanced view between operationalizing AI and building AI models. Practicality: The \u0026ldquo;Live Demo\u0026rdquo; of building a GenAI chatbot was incredibly valuable. It showed that building a functional prototype can take hours, not months. Networking: The ice-breaker activity and networking session allowed me to discuss the current demand for \u0026ldquo;AI-literate developers\u0026rdquo; in the Vietnamese market. Some Event Photos This event bridged the gap between traditional software development and modern AI. It confirmed that integrating GenAI is the next logical step for enhancing the user experience in my current projects.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Introducing Strands Agents, an Open Source AI Agents SDK Strands Agents is an open-source SDK that simplifies building and deploying AI agents using a model-driven approach, requiring just a few lines of code. It powers production systems at AWS teams like Amazon Q Developer and AWS Glue, enabling faster development from months to days while leveraging LLMs for planning, tool calls, and reflection. Key features include seamless tool integration (20+ pre-built tools and thousands via MCP), support for multiple models (Bedrock, Claude, Llama, Ollama), and production-ready deployment on AWS services with OpenTelemetry observability.\nBlog 2 - Introducing managed accounting for AWS Parallel Computing Service AWS Parallel Computing Service (PCS) introduces managed accounting to simplify HPC management with Slurm, offering resource monitoring, usage limits, and project attribution for chargeback and budgeting. This eliminates the need for separate databases, providing clear visibility into \u0026ldquo;who did what\u0026rdquo; for better reporting and capacity planning. Enable it during cluster setup with Slurm 24.11+, configure via console, and leverage use cases like job attribution, CPU limits to prevent hoarding, utilization reports, and failure analysis. Pricing includes hourly fees by cluster size plus GB-month storage—start in the AWS PCS console for efficient, scalable HPC operations.\nBlog 3 - Simplify AWS AppSync Events integration with Powertools for AWS Lambda AWS AppSync Events powers real-time WebSocket features, and the latest Powertools for AWS Lambda introduces AppSyncEventsResolver for Python, TypeScript, and .NET—streamlining event handling with pattern routing, filtering, transformation, and error management. Set up handlers for PUBLISH and SUBSCRIBE using intuitive channels and wildcards, slashing boilerplate code while ensuring robust authorization, full event access, and optimized aggregation. This lets developers focus on business logic without Lambda disruptions, perfect for chat apps, live dashboards, or IoT systems—boosting performance in real-time experiences.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Cloud Mastery Series #2 – DevOps on AWS Event Objectives Understand the core principles of DevOps culture and key performance metrics (DORA). Master the AWS DevOps ecosystem for building end-to-end CI/CD pipelines. Compare and select appropriate Infrastructure as Code (IaC) tools (CloudFormation vs. AWS CDK). Explore containerization strategies using Amazon ECS, EKS, and App Runner. Implement full-stack observability using CloudWatch and AWS X-Ray. Time \u0026amp; Venue Time: 08:30 – 17:00, Monday, November 17, 2025\nVenue: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Truong Quang Tinh - DevOps Engineer, TymeX | AWS Community Builder Kha Van - Cloud Security Engineer | AWS Community Builders Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Key Highlights 1. DevOps Culture \u0026amp; Metrics Cultural Shift: Moving from siloed development and operations to a unified culture of shared responsibility. Key Metrics (DORA): Focusing on four critical indicators to measure performance: Deployment Frequency (DF) Lead Time for Changes (LT) Mean Time to Restore (MTTR) Change Failure Rate (CFR) 2. AWS DevOps Services – CI/CD Pipeline Source Control: Strategies for Git management (GitFlow vs. Trunk-based development) using AWS CodeCommit. Orchestration: Automating the release process with AWS CodePipeline. Deployment Strategies: Blue/Green: Reduces downtime and risk by running two identical environments. Canary: Rolls out changes to a small subset of users first. Rolling: Updates instances incrementally. 3. Infrastructure as Code (IaC) AWS CloudFormation: The declarative approach using JSON/YAML templates to define infrastructure; utilizes drift detection to maintain state consistency. AWS CDK (Cloud Development Kit): The imperative approach allowing developers to define cloud resources using familiar programming languages (TypeScript, Python, Java). CDK constructs allow for reusable infrastructure patterns. 4. Container Services \u0026amp; Observability Compute Evolution: Selection criteria between Amazon ECS (tight control), Amazon EKS (Kubernetes standard), and AWS App Runner (simplified for developers). Observability: Going beyond simple monitoring by integrating CloudWatch (Logs, Metrics, Alarms) with AWS X-Ray for distributed tracing, enabling root-cause analysis in microservices. Key Takeaways Design Mindset Automation First: Manual processes are error-prone; everything from testing to infrastructure provisioning should be automated. Shift-Left Security: Integrating security checks early in the CI/CD pipeline rather than at the end. Measure Everything: You cannot improve what you do not measure. DORA metrics are the compass for DevOps maturity. Technical Architecture Immutable Infrastructure: Servers are never modified after deployment; they are replaced. Pipeline Orchestration: A robust pipeline must include build, unit test, integration test, and approval gates before production deployment. Traceability: In distributed systems, correlating logs and traces (via X-Ray) is mandatory for debugging. Modernization Strategy Right-sizing Compute: Use App Runner for simple web apps to reduce operational overhead, and reserve EKS for complex, orchestrated microservices. IaC Adoption: Moving away from manual console clicks to code-defined infrastructure to ensure reproducibility and disaster recovery. Applying to Work Adopt Trunk-based Development: Streamline the git workflow in the team project to reduce merge conflicts and accelerate integration. Implement CI/CD: Build a pipeline using AWS CodePipeline that automatically deploys the student management backend upon pushing to the main branch. Migrate to IaC: Refactor the current manual setup of DynamoDB and Lambda functions into AWS CDK scripts for better maintainability. Enhance Monitoring: Set up CloudWatch Alarms for critical API errors (5xx) to proactively detect issues before the demo. Event Experience Attending the “DevOps on AWS” workshop was instrumental in bridging the gap between coding and operations. It provided a roadmap for building reliable, scalable systems. Key experiences included:\nLearning from practitioners: The speakers, being active AWS Community Builders, shared \u0026ldquo;war stories\u0026rdquo; and real-world pitfalls in DevOps, not just textbook theory. I gained clarity on the Shared Responsibility Model within a DevOps context. Hands-on technical exposure: Participated in a live CI/CD walkthrough, witnessing raw code transform into a deployed application via an automated pipeline in minutes. Experienced the power of AWS CDK by deploying a VPC and ECS cluster with significantly fewer lines of code compared to raw CloudFormation templates. Visualized distributed tracing with AWS X-Ray, understanding how to pinpoint latency bottlenecks in microservices. Networking and discussions: Engaged in discussions about the trade-offs between GitFlow and Trunk-based development for small teams and received guidance on the AWS Certified DevOps Engineer – Professional certification roadmap. Lessons learned IaC is non-negotiable: For any long-term project, infrastructure must be code. Observability is crucial: Building a system is half the battle; knowing it is healthy is the other half. Culture over tools: Tools like Jenkins or CodePipeline are useless without a culture of collaboration and continuous improvement. Some event photos Overall, the event not only provided technical skills in AWS tools but also instilled a professional DevOps mindset, preparing me to build production-grade systems efficiently.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "During the First Cloud Journey internship (08/09/2025 – 28/12/2025), I actively participated in five professional events organized by AWS Vietnam and its partners. These events not only helped me stay updated with the latest technology trends but also expanded my network and strengthened my practical knowledge.\nEvent 1 Event Name: Kick-off AWS First Cloud Journey Workforce – OJT FALL 2025\nTime: 08:30 – 12:00, Saturday, 06/09/2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Vietnam Cloud Day 2025 – Ho Chi Minh City Connect Edition for Builders\nTime: 09:00 – 17:00, Thursday, 18/09/2025\nLocation: 36th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee (participated in track: Migration \u0026amp; Modernization track)\nEvent 3 Event Name: AWS Cloud Mastery Series #1 – AI/ML \u0026amp; Generative AI\nTime: 08:30 – 12:00, Saturday, 15/11/2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nTime: 08:30 – 17:00, Monday, 17/11/2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #3 – Cloud Security Best Practices\nTime: 08:30 – 12:00, Saturday, 29/11/2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Cloud Mastery Series #3 - ​Theo AWS Well-Architected Security Pillar Event Objectives Deeply understand the Security Pillar within the AWS Well-Architected Framework. Master the core principles: Least Privilege, Zero Trust, and Defense in Depth. Learn how to implement security controls across 5 key areas: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Identify top security threats in the cloud environment specifically for the Vietnamese market. Time \u0026amp; Venue Time: 08:30 – 12:00, Saturday, November 29, 2025\nVenue: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Le Vu Xuan An - Software \u0026amp; Cloud Engineer | AWS Cloud Club Captain HCMUTE Tran Doan Cong Ly - DevOps Engineer | AWS FCAJ Ambassador | AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud | AWS First Cloud AI Journey | AWS Cloud Club Captain HUFLIT Tran Duc Anh - Cloud Security Engineer Trainee | First Cloud AI Journey | AWS Cloud Club Captain SGU Nguyen Tuan Thinh - Cloud Engineer Trainee | First Cloud AI Journey Nguyen Do Thanh Dat - Cloud Engineer Trainee | First Cloud AI Journey Đinh Le Hoang Anh – Cloud Engineer Trainee | First Cloud AI Journey Kha Van - Cloud Security Engineer | AWS Community Builders Special Guest:\nMendel Grabski - Cloud Security \u0026amp; Solution Architect | Enabling Secure-by-Design Solutions Truong Quang Tinh - DevOps Engineer, TymeX | AWS Community Builder Key Highlights 1. Security Foundations \u0026amp; Identity (IAM) Core Principles: Moved beyond perimeter security to a Zero Trust model where every request must be authenticated and authorized. Modern IAM: Transition from long-term credentials (IAM Users with Access Keys) to temporary credentials via IAM Roles and Identity Center (SSO). Enforcing Least Privilege using Access Analyzer to validate policies. Multi-Account Strategy: Using SCPs (Service Control Policies) to set permission boundaries across the organization. 2. Detection \u0026amp; Continuous Monitoring Centralized Visibility: utilizing Security Hub to aggregate alerts from GuardDuty, Inspector, and Macie. Logging Strategy: Enabling logs at all layers is non-negotiable (VPC Flow Logs for network, CloudTrail for API calls, S3 Server Access Logs). Detection-as-Code: Defining alert rules as code to ensure consistent monitoring across environments. 3. Infrastructure \u0026amp; Data Protection Network Security: Implementing Defense in Depth with VPC segmentation (Public/Private subnets), Security Groups (stateful), and WAF/Shield for edge protection. Encryption: At-rest: Using KMS with automatic key rotation for EBS, RDS, and S3. In-transit: Enforcing TLS 1.2+ for all data movement. Secrets Management: replacing hardcoded credentials in code with AWS Secrets Manager or Parameter Store. 4. Incident Response (IR) Automation IR Lifecycle: Preparation → Detection \u0026amp; Analysis → Containment, Eradication \u0026amp; Recovery → Post-Incident Activity. Playbooks: Detailed walkthroughs for handling common scenarios like compromised IAM keys or public S3 buckets. Automation: Using Lambda and Step Functions to auto-remediate issues (e.g., automatically revoking a compromised user\u0026rsquo;s session). Key Takeaways Security Mindset Security is everyone\u0026rsquo;s responsibility: It is not just for the security team; developers must practice \u0026ldquo;Security by Design\u0026rdquo;. Assume Breach: Always design systems assuming that a component might be compromised, limiting the \u0026ldquo;blast radius\u0026rdquo;. Technical Architecture Identity as the new perimeter: In a cloud-native world, IAM is more critical than the network firewall. Immutable Infrastructure: Patching servers in place is risky; prefer replacing them with new, patched images to prevent malware persistence. Applying to Work Refactor IAM: Immediately review the group project\u0026rsquo;s IAM policies. Remove any *:* permissions and replace hardcoded Access Keys with IAM Roles for EC2/Lambda. Secure Secrets: Migrate database credentials from .env files to AWS Systems Manager Parameter Store. Enable GuardDuty: Activate GuardDuty in the project account to detect anomalies (cryptojacking, unauthorized access) during the development phase. Encryption: Enable default encryption (SSE-S3) for all S3 buckets used in the Student Management System project. Event Experience The workshop provided a concentrated, high-intensity dive into cloud security. Unlike general overviews, this session focused on actionable patterns:\nInteractive Learning: The mini-demo on validating IAM Policies and simulating access helped visualize how easily permissions can be misconfigured. Real-world relevance: Discussing \u0026ldquo;Top threats in Vietnam\u0026rdquo; made the content highly relatable, emphasizing the need to protect against common misconfigurations and credential leaks. Practical Automation: Seeing an automated IR playbook triggered by EventBridge and executed by Lambda changed my perspective on how to handle security incidents at scale. Some Event Photos This event shifted my perspective from \u0026ldquo;building it fast\u0026rdquo; to \u0026ldquo;building it secure.\u0026rdquo; I now have a clear roadmap to harden our applications before they reach production.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "From September 08, 2025 to December 28, 2025, I had the honor of participating in the First Cloud Journey program organized by AWS Vietnam. This was an invaluable opportunity for me to gain hands-on experience with cloud technology, contribute to building and deploying solutions on the AWS platform, and work alongside experienced mentors and talented fellow interns.\nThrough weeks of learning, hands-on practice, and the execution of our group project — Serverless Student Management System, a fully serverless student management platform (similar to university LMS systems) — I significantly strengthened and expanded my knowledge of AWS services including Route 53, Cognito, Amplify, WAF, API Gateway, DynamoDB, S3, EventBridge, AppSync (GraphQL), SES, CloudFront, and CloudWatch. At the same time, I improved my programming skills, teamwork abilities, presentation skills, and gained practical experience working with GitLab.\nRegarding work attitude, I consistently maintained a serious learning mindset, fully complied with the program’s rules and processes, proactively collaborated with team members, and strictly respected deadlines and established workflows.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Excellent Good Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving mindset Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/organization Work effectiveness, improvement initiatives, and Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Areas for Future Improvement Further strengthen personal discipline, especially regarding punctuality and time management, particularly under high-pressure deadlines toward the end of the program. Enhance presentation skills and professional English communication in an international working environment. Develop stronger systems thinking and advanced debugging capabilities to resolve production-level issues more quickly and accurately. Take a more proactive role in code reviews and knowledge-sharing sessions to build technical leadership skills in the future. The First Cloud Journey program has truly been a major milestone in my academic and professional journey. I am deeply grateful to AWS Vietnam, the mentors, and my teammates for their guidance and companionship throughout this meaningful period.\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/7-feedback/",
	"title": "Sharing &amp; Feedback",
	"tags": [],
	"description": "",
	"content": "General Evaluation 1. Working Environment\nThe environment at FCJ is highly dynamic and values a spirit of ownership. The working atmosphere is professional yet comfortable enough for me to openly exchange ideas with mentors. However, the noise level when crowded can be distracting at times, but the policy allowing headphones has effectively solved this issue.\n2. Support from Mentor / Team Admin\nThe mentors are extremely dedicated and always available when support is needed. I highly appreciate that they encourage me to try and solve problems independently rather than just providing the answers immediately.\n3. Relevance between work and major\nThe assigned tasks have a progressive level of difficulty, which is very suitable for the roadmap of a 3rd or 4th-year student. I have had the opportunity to apply foundational knowledge of Programming and Computer Networking learned at university to a real-world Cloud environment.\n4. Learning opportunities \u0026amp; Skill development\nDuring the internship, I learned many new skills such as using project management tools, teamwork, and professional communication within a corporate environment. Mentors also shared a lot of practical experience, helping me better orient my future career path.\n5. Culture \u0026amp; Team Spirit\nFrom the beginning of the First Cloud Journey, I was very impressed by the community\u0026rsquo;s team spirit. Everyone respects one another; we work seriously but maintain a fun atmosphere. Support is given regardless of whether one is an intern or a full-time employee, which makes me feel like a true part of the collective.\n6. Policies / Benefits for Interns\nI particularly enjoy the Workshops/Events organized by AWS; these help me and other students access new knowledge and gain opportunities to network with seniors in the field. Additionally, the Hybrid working model provides favorable conditions for time flexibility and comfort.\nOther Questions What are you most satisfied with during the internship? I am most satisfied with the program\u0026rsquo;s policies and regulations, which create very favorable conditions for participants. Additionally, the friendliness and readiness of mentors to help and answer questions is a highlight.\nWhat do you think the company needs to improve for future interns? The documentation is very deep and extensive, so I was quite confused in the beginning. I hope the team can update the guidance to provide a more detailed direction/roadmap.\nIf you were to introduce this to friends, would you recommend interning here? Why? Definitely yes. This is an ideal place to experience work culture. The company organizes many events that provide both new knowledge and job opportunities for students. Especially, the mentors are all dedicated to answering questions and supporting students.\nProposals \u0026amp; Desires Do you have any suggestions to improve the internship experience? Since most interns are about to graduate, I suggest the team organize additional workshops/talkshows regarding Job Interviews, working experiences, or in-depth CV reviews. This would help interns review missing knowledge and feel more confident when applying for official positions at the company\nDo you want to continue this program in the future? Yes, I really hope to continue accompanying the First Cloud Journey program in the future.\nOther comments (feel free to share):\n"
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-fall2025-internship-report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]